# Voice Memory - 产品白皮书

**版本：** v2.0 (Pivot)
**日期：** 2025-12-31
**定位：** Voice Interface for Your Digital Workspace (数字工作空间的语音接口)

---

## 一、 产品愿景 (The Vision)

### 1.1 核心理念
Voice Memory 不仅仅是一个聊天机器人，它是**极客的第二大脑**。它是一个运行在本地的、拥有声音的智能代理，能够深度理解你的数字工作空间（代码、笔记、文档），并通过自然的语音交互即时响应你的思考与指令。

### 1.2 解决什么痛点？
*   **交互的割裂**: 知识在本地（IDE, Obsidian），语音助手在云端（Siri, ChatGPT）。两者无法互通。
*   **效率的瓶颈**: 传统的 "Text-to-Code" 工具（如 Claude Code）虽然强大，但依然是“冷”的文本交互，无法满足散步、构思或双手被占用时的“心流”状态。
*   **隐私的顾虑**: 极其私人的想法和核心代码，不应无脑上传到云端黑盒。

### 1.3 核心价值三角
1.  **Interaction (交互)**: **可打断的自然语音**。打破回合制，实现毫秒级响应的 Barge-in 体验。
2.  **Context (上下文)**: **深度本地感知**。不仅仅是对话历史，更能感知当前工作目录、文件内容和系统状态。
3.  **Memory (记忆)**: **私有长期记忆**。基于本地向量库 (RAG)，随着使用时间增长，越用越懂你。

---

## 二、 演进路线图 (Roadmap)

我们采取 **"Kernel -> Brain -> Hands"** 的演进策略。

### Phase 1: MVP (已完成)
*   **目标**: 验证 Web 语音交互闭环。
*   **状态**: ✅ 功能跑通（录音 -> STT -> LLM -> TTS）。
*   **局限**: 线性流程，不可打断，无深度上下文。

### Phase 2: The Kernel (当前阶段 - 核心重构)
*   **目标**: **实现"像人一样交流"的实时感**。构建可扩展的 Pipeline 架构。
*   **核心特性**:
    *   **Barge-in (可打断)**: 前端 VAD + 后端即时中断机制。
    *   **Pipeline Architecture**: 将后端重构为模块化流水线 (STT -> Intent -> LLM -> TTS)。
    *   **Streaming**: 全链路流式处理，降低延迟。
*   **交付物**: 一个极速、流畅、可打断的 Web PWA 语音助手。

### Phase 3: The Brain (深度记忆)
*   **目标**: **实现"不仅能聊，还能懂我"**。
*   **核心特性**:
    *   **Local RAG**: 集成向量数据库 (Qdrant/SQLite-VSS)。
    *   **Memory Processor**: 自动将对话沉淀为长期记忆。
    *   **Knowledge Graph**: (探索中) 建立知识点之间的关联。

### Phase 4: The Hands (工具集成)
*   **目标**: **实现"不仅能懂，还能干活"**。
*   **核心特性**:
    *   **Tool Use (MCP/Function Call)**: 安全地执行本地工具 (ls, grep, cat)。
    *   **FileSystem Access**: 读取/分析本地代码和文档。
    *   **Agentic Workflow**: 复杂的任务编排（如"分析这个报错并给出修复建议"）。
*   **技术路径**: LLM (Cloud) <--> Function Calling <--> Local Executor (Go) <--> File System

---

## 三、 交互形态

| 形式 | 定位 | 优先级 |
| :--- | :--- | :--- |
| **Web PWA** | **核心交互端**。利用浏览器强大的 AudioWorklet 能力提供最佳体验。 | **P0** |
| **Terminal CLI** | **辅助端**。用于 Headless 模式或纯键盘场景。 | P1 |
| **Native App** | 深度集成端。未来考虑，目前暂缓。 | P2 |

---

## 四、 技术架构原则

1.  **Pipeline First**: 一切皆插件。STT, LLM, Memory 都是 Pipeline 中的 Processor。
2.  **Local Context**: 后端 (Go) 必须运行在本地 (Localhost)，守护用户的文件系统。
3.  **Model Agnostic**: 架构上不绑定特定模型。支持百度/GLM (低成本/国内)，也支持 OpenAI/DeepSeek (高性能)。

---

## 五、 关于 Phase 4 本地文件处理的思考

**Q: 云端 LLM 如何操作本地文件？**

**A: "Local Proxy" 模式**

1.  **用户说**: "帮我看看当前目录下的 main.go 有什么问题。"
2.  **LLM (云端)**: 收到文本，判断需要读取文件。
3.  **Function Call (云端)**: 返回一个结构化指令 `call_function: { name: "read_file", path: "./main.go" }`。
4.  **Local Backend (Go)**:
    *   接收到这个指令。
    *   **安全检查**: 确认路径在允许范围内 (Sandbox)。
    *   **执行**: 读取本地磁盘上的 `main.go` 内容。
    *   **回传**: 将文件内容作为 "Tool Output" 发送回 LLM。
5.  **LLM (云端)**: 结合文件内容，生成分析结果。
6.  **TTS**: 播放语音 "我看了代码，第 20 行有个逻辑错误..."

**核心机制**:
*   **MCP (Model Context Protocol)**: 未来可标准化的协议。
*   **Function Calling**: 现有的成熟机制。
*   **Go Backend**: 作为 **"本地代理人" (Local Agent)**，它是连接云端大脑和本地磁盘的桥梁。

---