# Voice Memory - 可打断技术实现方案

**版本：** v0.1
**日期：** 2025-12-29
**状态：** 技术设计阶段

---

## 目录

1. [核心概念](#一核心概念)
2. [Phase 1: 文本中介架构](#二phase-1-文本中介架构)
3. [Phase 2: 可打断实现](#三phase-2-可打断实现)
4. [Phase 3: 端到端语音](#四phase-3-端到端语音)
5. [技术选型对比](#五技术选型对比)
6. [性能优化](#六性能优化)

---

## 一、核心概念

### 1.1 什么是"可打断"（Barge-in）

```
传统对话（轮流）：
  User ████████                    ████████
  AI            ████████████████████

可打断对话：
  User ████████   ████         ███████████
  AI      ██████████    ████████
              ↑打断
```

**关键特性：**
1. **双工音频：** 同时录音和播放
2. **打断检测：** VAD（Voice Activity Detection）
3. **快速响应：** 检测到打断后<1秒停止
4. **上下文保持：** 打断后记得之前内容

### 1.2 技术挑战

| 挑战 | 描述 | 解决方案 |
|------|------|----------|
| **回声消除** | 播放的声音被录音录进去 | AEC（Acoustic Echo Cancellation）|
| **打断检测** | 区分背景噪音和真实打断 | VAD + 多阈值判断 |
| **延迟控制** | 打断后快速响应 | 流水线优化、并行处理 |
| **上下文保持** | 打断后无缝衔接 | 会话状态管理 |

---

## 二、Phase 1: 文本中介架构

### 2.1 架构图

```
┌─────────────────────────────────────────────────────────────────┐
│                    Phase 1: 基础流程                           │
└─────────────────────────────────────────────────────────────────┘

用户说话
   │
   ▼
[麦克风] → 录音(5秒)
   │
   ▼
[STT] → OpenAI Whisper API
   │
   ▼
文本 → "帮我查一下天气"
   │
   ▼
[LLM] → Claude 3.5 Haiku
   │
   ▼
文本 ← "好的，我帮你查今天..."
   │
   ▼
[TTS] → OpenAI TTS API
   │
   ▼
[扬声器] → 播放音频

延迟: 2-3秒
```

### 2.2 核心代码

```python
# phase1_basic_conversation.py

import asyncio
from core.stt import SpeechToText
from core.tts import TextToSpeech
from core.ai_client import AIClient

class BasicConversation:
    """基础对话（Phase 1）"""

    def __init__(self):
        self.stt = SpeechToText()
        self.tts = TextToSpeech()
        self.ai = AIClient()

    async def run_turn(self, audio_data: bytes) -> bytes:
        """执行一轮对话"""
        # 1. STT
        text = await self.stt.transcribe_async(audio_data)
        print(f"用户: {text}")

        # 2. AI对话
        response = self.ai.chat([{"role": "user", "content": text}])
        print(f"AI: {response}")

        # 3. TTS
        audio = await self.tts.synthesize_async(response)

        return audio

    async def playback(self, audio_data: bytes):
        """播放音频"""
        import pyaudio
        p = pyaudio.PyAudio()

        stream = p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=24000,
            output=True
        )

        stream.write(audio_data)
        stream.close()
        p.terminate()

# 使用示例
async def main():
    conv = BasicConversation()

    # 录音5秒
    audio = await record_audio(5)

    # 对话
    response_audio = await conv.run_turn(audio)

    # 播放
    await conv.playback(response_audio)
```

**特点：**
- ✅ 简单可靠
- ✅ 快速验证
- ❌ 不可打断
- ❌ 延迟较高（2-3秒）

---

## 三、Phase 2: 可打断实现

### 3.1 架构图

```
┌─────────────────────────────────────────────────────────────────┐
│                Phase 2: 可打断对话架构                          │
└─────────────────────────────────────────────────────────────────┘

                        ┌─────────────┐
                        │  状态管理器  │
                        │ State       │
                        │  Manager    │
                        └──────┬───────┘
                               │
        ┌──────────────────────┼──────────────────────┐
        │                      │                      │
        ▼                      ▼                      ▼
   ┌─────────┐          ┌─────────┐          ┌─────────┐
   │ 录音线程 │          │ TTS线程  │          │ VAD监听  │
   │ Recording│          │ Playback │          │ Barge-in │
   └────┬────┘          └────┬────┘          └────┬────┘
        │                     │                     │
        │                     │                     │
        ▼                     ▼                     ▼
   [流式STT]            [流式TTS]            [打断检测]
        │                     │                     │
        └─────────────────────┴─────────────────────┘
                              │
                              ▼
                        [LLM Claude]
```

### 3.2 核心组件

#### 3.2.1 状态管理器

```python
# phase2/state_manager.py

from enum import Enum
import asyncio

class ConversationState(Enum):
    """对话状态"""
    IDLE = "idle"              # 空闲
    LISTENING = "listening"    # 聆听中
    PROCESSING = "processing"  # AI思考中
    SPEAKING = "speaking"      # AI说话中
    INTERRUPTED = "interrupted" # 被打断

class StateManager:
    """状态管理器"""

    def __init__(self):
        self.state = ConversationState.IDLE
        self.state_lock = asyncio.Lock()
        self.context = []  # 对话上下文
        self.interrupted_content = ""  # 被打断的内容

    async def transition_to(self, new_state: ConversationState):
        """状态转移"""
        async with self.state_lock:
            old_state = self.state
            self.state = new_state
            print(f"[状态] {old_state} → {new_state}")

    def can_listen(self) -> bool:
        """是否可以开始聆听"""
        return self.state in [ConversationState.IDLE, ConversationState.SPEAKING]

    def can_interrupt(self) -> bool:
        """是否可以被打断"""
        return self.state == ConversationState.SPEAKING

    def save_interrupted_content(self, content: str):
        """保存被打断的内容"""
        self.interrupted_content = content

    def get_interrupted_context(self) -> str:
        """获取被打断的上下文"""
        return self.interrupted_content
```

#### 3.2.2 VAD打断检测

```python
# phase2/vad_detector.py

import numpy as np

class VADDetector:
    """语音活动检测器"""

    def __init__(
        self,
        sample_rate: int = 16000,
        frame_duration: int = 30,  # ms
        threshold: float = 0.5,
        min_speech_duration: float = 0.5  # 最小说话时长
    ):
        self.sample_rate = sample_rate
        self.frame_size = int(sample_rate * frame_duration / 1000)
        self.threshold = threshold
        self.min_speech_frames = int(min_speech_duration * 1000 / frame_duration)

        self.speech_frame_count = 0
        self.is_speech = False

    def process_frame(self, audio_frame: np.ndarray) -> bool:
        """
        处理一帧音频

        Returns:
            bool: 是否检测到语音活动
        """
        # 计算能量
        energy = np.mean(audio_frame ** 2)

        # 归一化
        energy_db = 10 * np.log10(energy + 1e-10)
        normalized = max(0, min(1, (energy_db + 100) / 100))

        # 判断是否为语音
        if normalized > self.threshold:
            self.speech_frame_count += 1
        else:
            self.speech_frame_count = 0

        # 状态转换
        if not self.is_speech and self.speech_frame_count > self.min_speech_frames:
            self.is_speech = True
            return True  # 检测到语音开始
        elif self.is_speech and self.speech_frame_count == 0:
            self.is_speech = False
            return False  # 语音结束

        return self.is_speech
```

#### 3.2.3 可打断对话管理器

```python
# phase2/interruptible_conversation.py

import asyncio
import pyaudio
from phase2.state_manager import StateManager, ConversationState
from phase2.vad_detector import VADDetector
from core.stt import SpeechToText
from core.tts import TextToSpeech
from core.ai_client import AIClient

class InterruptibleConversation:
    """可打断对话管理器"""

    def __init__(self):
        self.state_manager = StateManager()
        self.vad = VADDetector()
        self.stt = SpeechToText()
        self.tts = TextToSpeech()
        self.ai = AIClient()

        # 音频
        self.audio = pyaudio.PyAudio()
        self.recording_stream = None
        self.playback_stream = None

        # 控制
        self.is_running = False
        self.should_stop_speaking = asyncio.Event()

    async def start(self):
        """启动对话循环"""
        self.is_running = True
        await self.state_manager.transition_to(ConversationState.LISTENING)

        # 启动监听线程
        asyncio.create_task(self._listen_loop())

        # 启动打断检测线程
        asyncio.create_task(self._vad_loop())

    async def _listen_loop(self):
        """聆听循环"""
        while self.is_running:
            if self.state_manager.can_listen():
                await self.state_manager.transition_to(ConversationState.LISTENING)

                # 开始录音
                audio_data = await self._record_audio()

                # STT
                await self.state_manager.transition_to(ConversationState.PROCESSING)
                text = await self.stt.transcribe_async(audio_data)

                # AI对话
                response = await self._ai_respond(text)

                # TTS播放
                await self._speak(response)

    async def _vad_loop(self):
        """VAD检测循环（后台运行）"""
        while self.is_running:
            if self.state_manager.can_interrupt():
                # 检测是否用户在说话
                if self._detect_user_speech():
                    print("检测到打断！")
                    await self._handle_interruption()

            await asyncio.sleep(0.1)  # 100ms检测一次

    async def _record_audio(self, duration: float = 5.0) -> bytes:
        """录音"""
        frames = []
        stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=16000,
            input=True,
            frames_per_buffer=1024
        )

        for _ in range(int(16000 * duration / 1024)):
            data = stream.read(1024, exception_on_overflow=False)
            frames.append(data)

        stream.stop_stream()
        stream.close()

        return b''.join(frames)

    def _detect_user_speech(self) -> bool:
        """检测用户是否在说话（简化版）"""
        # 实际应该从录音流中实时检测
        # 这里用简化版：检测麦克风输入
        import audioop
        import sounddevice as sd

        def audio_callback(indata, frames, time, status):
            # 分析音量
            volume = np.mean(np.abs(indata))
            return volume > 0.01  # 简化阈值

        # 实际实现需要持续监听
        return False  # 占位

    async def _handle_interruption(self):
        """处理打断"""
        # 1. 停止TTS播放
        self.should_stop_speaking.set()

        # 2. 保存被打断的内容
        await self.state_manager.transition_to(ConversationState.INTERRUPTED)

        # 3. 停止当前对话
        self.is_running = False

        # 4. 重新开始聆听
        await asyncio.sleep(0.5)  # 等待用户说完
        await self.start()

    async def _speak(self, text: str):
        """说话（支持被打断）"""
        await self.state_manager.transition_to(ConversationState.SPEAKING)

        # 流式TTS
        self.should_stop_speaking.clear()

        # 创建播放流
        self.playback_stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=24000,
            output=True
        )

        # 分段生成和播放
        sentences = self._split_sentences(text)

        for sentence in sentences:
            if self.should_stop_speaking.is_set():
                print("[打断] 停止播放")
                break

            # 生成语音
            audio = await self.tts.synthesize_async(sentence)

            # 播放
            self.playback_stream.write(audio)

            # 同时检测打断
            if self._detect_user_speech():
                await self._handle_interruption()
                break

        self.playback_stream.close()

    async def _ai_respond(self, user_input: str) -> str:
        """AI回复"""
        # 获取上下文
        context = self.state_manager.context
        context.append({"role": "user", "content": user_input})

        # 调用AI
        response = self.ai.chat(context)

        # 保存回复
        context.append({"role": "assistant", "content": response})

        return response

    @staticmethod
    def _split_sentences(text: str) -> list:
        """分段（简化版）"""
        import re
        # 按句子分割
        sentences = re.split(r'[。！？.!?]', text)
        return [s + '。' for s in sentences if s.strip()]

    async def stop(self):
        """停止对话"""
        self.is_running = False

        if self.recording_stream:
            self.recording_stream.close()
        if self.playback_stream:
            self.playback_stream.close()

        self.audio.terminate()
```

### 3.3 关键技术点

#### 3.3.1 双工音频

```python
# 使用PyAudio实现双工音频

import pyaudio

class DuplexAudio:
    """双工音频管理"""

    def __init__(self):
        self.audio = pyaudio.PyAudio()
        self.duplex_stream = None

    def start_duplex(self):
        """启动双工模式（同时录音和播放）"""
        self.duplex_stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=16000,
            input=True,
            output=True,
            frames_per_buffer=1024
        )

    def read_audio(self, num_frames: int = 1024) -> bytes:
        """读取音频"""
        return self.duplex_stream.read(num_frames, exception_on_overflow=False)

    def write_audio(self, audio_data: bytes):
        """写入音频"""
        self.duplex_stream.write(audio_data)

    def stop(self):
        """停止"""
        if self.duplex_stream:
            self.duplex_stream.close()
        self.audio.terminate()
```

#### 3.3.2 回声消除

```python
# 使用py-webrtcvad进行回声消除

import webrtcvad

class EchoCanceller:
    """回声消除器"""

    def __init__(self):
        self.vad = webrtcvad.Vad(2)  # aggressiveness: 0-3

    def is_speech(self, frame: bytes, sample_rate: int) -> bool:
        """判断是否为语音（非回声）"""
        return self.vad.is_speech(frame, sample_rate)

    def filter(self, audio_data: bytes, sample_rate: int = 16000) -> bool:
        """过滤回声，返回是否为真实语音"""
        frame_duration = 30  # ms
        frame_size = int(sample_rate * frame_duration / 1000)

        # 分帧处理
        for i in range(0, len(audio_data), frame_size):
            frame = audio_data[i:i + frame_size]
            if len(frame) < frame_size:
                break

            if self.is_speech(frame, sample_rate):
                return True

        return False
```

---

## 四、Phase 3: 端到端语音

### 4.1 架构对比

```
┌─────────────────────────────────────────────────────────────────┐
│              Phase 2 vs Phase 3 架构对比                        │
└─────────────────────────────────────────────────────────────────┘

Phase 2: 文本中介（可打断）
  音频 → STT → 文本 → LLM → 文本 → TTS → 音频
   │      │      │      │      │      │
   └─────┼──────┼──────┼──────┼──────┘
          │      │      │      │
         2-3秒延迟

Phase 3: 端到端语音
  音频 → [语音模型] → 音频
   │                  │
   └────── <500ms ──────┘
```

### 4.2 OpenAI Realtime API

```python
# phase3/realtime_conversation.py

import asyncio
from openai import AsyncOpenAI

class RealtimeConversation:
    """端到端语音对话"""

    def __init__(self, api_key: str):
        self.client = AsyncOpenAI(api_key=api_key)

    async def connect(self):
        """建立WebSocket连接"""
        # OpenAI Realtime API使用WebSocket
        # 这里是伪代码示例
        pass

    async def send_audio(self, audio_data: bytes):
        """发送音频"""
        # 直接发送音频数据
        await self.ws.send(audio_data)

    async def receive_audio(self) -> bytes:
        """接收音频"""
        # 接收AI的音频回复
        message = await self.ws.recv()
        return message

    async def run(self):
        """运行对话循环"""
        async with self.client.audio.stream.with_web_socket() as ws:
            self.ws = ws

            # 发送音频配置
            await self.ws.send_json({
                "type": "input_audio_buffer.append",
                "audio": base64_audio
            })

            # 接收响应
            async for message in ws:
                if message.type == "audio":
                    # 直接播放音频
                    await self.play_audio(message.audio)
                elif message.type == "input_audio.commit":
                    # 用户说完了
                    pass

    async def play_audio(self, audio_data: bytes):
        """播放音频"""
        # 直接播放接收到的音频
        pass
```

### 4.3 性能对比

| 指标 | Phase 2（文本中介） | Phase 3（端到端） |
|------|---------------------|-----------------|
| **延迟** | 1-2秒 | <500ms |
| **自然度** | 中高 | 极高 |
| **打断响应** | <1秒 | <300ms |
| **实现复杂度** | 中等 | 高 |
| **成本** | 较低 | 较高 |

---

## 五、技术选型对比

### 5.1 VAD方案

| 方案 | 准确度 | 延迟 | 复杂度 | 推荐度 |
|------|--------|------|--------|--------|
| **webrtcvad** | 高 | 极低 | 低 | ⭐⭐⭐⭐⭐ |
| **silero-vad** | 极高 | 低 | 中 | ⭐⭐⭐⭐ |
| **pyannote-audio** | 高 | 中 | 高 | ⭐⭐⭐ |
| **自研** | 可控 | 可控 | 极高 | ⭐⭐ |

**推荐：** webrtcvad（Phase 2），silero-vad（Phase 3）

### 5.2 流式TTS方案

| 方案 | 质量 | 延迟 | 成本 | 推荐度 |
|------|------|------|------|--------|
| **OpenAI TTS流式** | 高 | 中 | 低 | ⭐⭐⭐⭐⭐ |
| **ElevenLabs流式** | 极高 | 低 | 高 | ⭐⭐⭐⭐ |
| **Coqui流式** | 中高 | 低 | 免费 | ⭐⭐⭐ |
| **Azure流式** | 高 | 低 | 中 | ⭐⭐⭐⭐ |

**推荐：** OpenAI TTS流式（MVP），ElevenLabs（高质量）

### 5.3 实时语音API

| 方案 | 延迟 | 成本 | 可用性 | 推荐度 |
|------|------|------|--------|--------|
| **OpenAI Realtime API** | <300ms | 中 | Beta | ⭐⭐⭐⭐⭐ |
| **ElevenLabs Conversational** | <500ms | 高 | 可用 | ⭐⭐⭐⭐ |
| **Azure Realtime** | <500ms | 中 | 可用 | ⭐⭐⭐⭐ |
| **自研端到端** | 可控 | 高 | 研发中 | ⭐⭐⭐ |

**推荐：** OpenAI Realtime API（首选），自研（长期）

---

## 六、性能优化

### 6.1 延迟优化

```yaml
Phase 2优化策略:
  流水线并行:
    - STT和LLM并行准备
    - TTS预生成常用短语

  缓存策略:
    - 常用回复预生成
    - 音频片段缓存

  网络优化:
    - 保持连接池
    - 使用更近的服务器

Phase 3优化策略:
  边缘计算:
    - 本地运行小型模型
    - 云端处理复杂任务

  模型优化:
    - 量化模型
    - 剪枝优化
```

### 6.2 准确度优化

```yaml
打断检测优化:
  多阶段检测:
    - Stage 1: 快速VAD（低阈值）
    - Stage 2: STT验证
    - Stage 3: 意图分析

  误触发减少:
    - 去噪处理
    - 回声消除
    - 机器学习分类

上下文理解优化:
  打断意图识别:
    - 纠正型打断："不对，应该是..."
    - 补充型打断："还有..."
    - 引导型打断："换个方向..."
```

---

## 七、实施路线图

### 7.1 分阶段实施

```
Week 1-4: Phase 1 MVP
  ├─ 基础STT → LLM → TTS
  ├─ 简单对话流程
  └─ 延迟2-3秒（可接受）

Week 5-8: Phase 2 可打断
  ├─ VAD打断检测
  ├─ 流式TTS
  ├─ 双工音频
  └─ 延迟<1秒

Week 9-12: Phase 2 优化
  ├─ 上下文保持
  ├─ 智能打断判断
  └─ 用户体验优化

Week 13+: Phase 3 端到端
  ├─ OpenAI Realtime API集成
  ├─ 性能优化
  └─ 延迟<500ms
```

### 7.2 风险缓解

| 风险 | 缓解措施 |
|------|----------|
| VAD误触发 | 多阶段验证、阈值调优 |
| 延迟不达标 | 流水线优化、本地模型 |
| 回声问题 | AEC处理、硬件优化 |
| 上下文丢失 | 状态管理、会话恢复 |

---

**文档版本历史：**
- v0.1 (2025-12-29): 初始版本，可打断技术方案
