# Voice Memory - 产品文档

**版本：** v0.2
**日期：** 2025-12-29
**状态：** 概念设计阶段（更新：强调可打断特性）

---

## 一、产品概述

### 1.1 产品定位

**Voice Memory** 是一款**可打断的实时语音AI助手**，让用户能够像与人交谈一样自然地与AI对话——随时说话、随时打断、真正双向互动。

> **核心特性：可打断（Barge-in Capable）**
> 用户可以在AI说话的任何时候打断，AI立即停止并聆听，就像真人对话一样。

### 1.2 核心价值

> **"像和人说话一样与AI交流"**

```
传统AI语音助手                  Voice Memory
    │                              │
    ├─ 轮流说话，互不打断          ├─ 可随时打断，自然对话
    ├─ 说完才能听，机械生硬        ├─ 实时交互，像真人一样
    ├─ 对话结束，记忆消失          ├─ 对话持续积累，记忆永久保存
    ├─ 云端处理，隐私顾虑          ├─ 本地优先，隐私可控
    └─ 单次问答，无上下文          └─ 完整上下文，智能关联
```

### 1.3 产品演进路径

```
┌─────────────────────────────────────────────────────────────┐
│                     产品演进路线                              │
└─────────────────────────────────────────────────────────────┘

Phase 1: 文本中介（MVP，4-7周）
  STT → 文本 → LLM → 文本 → TTS
  ├─ 目标：快速验证核心交互
  ├─ 特性：基础对话、持久记忆
  └─ 延迟：2-3秒端到端

Phase 2: 可打断交互（8-12周）
  STT → 文本 → LLM → 文本 → TTS（流式 + 打断检测）
  ├─ 目标：实现自然的可打断对话
  ├─ 特性：实时STT、流式TTS、打断检测
  └─ 延迟：1秒内响应打断

Phase 3: 端到端语音（13-20周）
  音频 → 语音模型 → 音频
  ├─ 目标：原生语音到语音
  ├─ 特性：更低延迟、更自然
  └─ 延迟：<500ms
```

### 1.4 目标用户

| 用户类型 | 痛点 | 解决方案 |
|----------|------|----------|
| **知识工作者** | 大量思考碎片，缺乏系统整理 | 随时语音记录，自动归档结构化 |
| **研究人员** | 跨领域知识点关联困难 | 语义知识图谱自动连接 |
| **创作者** | 灵感和想法容易丢失 | 语音捕捉，双向协作增强 |
| **技术从业者** | 项目上下文频繁丢失 | 持久记忆，Terminal无缝集成 |
| **隐私敏感用户** | 不信任云端AI服务 | 本地优先，完全自托管 |

### 1.5 "可打断"特性详解

**什么是可打断？**
```
传统AI语音助手：
  用户: "帮我查一下..."         [必须说完]
  AI: "好的，我在查..."         [必须听完]
  用户: （只能等AI说完）         [无法打断]

Voice Memory（可打断）：
  用户: "帮我查一下..."
  AI: "好的，我在查天..."
  用户: "不对，查昨天的！"       [随时打断]
  AI: [立即停止] "明白了，查昨天的..."
```

**可打断的价值：**
- **自然流畅：** 像和人说话一样，不需要等对方说完
- **更正偏差：** AI理解错误时立即纠正，不用等它说一堆废话
- **快速引导：** AI方向错了时立刻调整，节省时间
- **情绪控制：** 用户觉得AI啰嗦时随时打断，体验更好

**技术实现层次：**
```
Level 1: 基础打断（MVP+）
  └─ 检测到用户说话就停止TTS播放

Level 2: 智能打断
  └─ 区分背景噪音和真实打断信号

Level 3: 上下文保留
  └─ 打断后恢复时记得之前说到哪

Level 4: 自然补全
  └─ AI根据打断内容智能补全思路
```

---

## 二、核心功能

### 2.1 功能矩阵

| 功能模块 | 核心能力 | 用户价值 |
|----------|----------|----------|
| **语音唤醒** | 自定义唤醒词，随时启动对话 | 无需手动操作，真正解放双手 |
| **可打断对话** | 随时打断AI说话，自然交互 | 像和人说话一样流畅 |
| **实时转录** | 边说边转文字，边听边显示 | 实时反馈，确认理解 |
| **持久记忆** | 跨会话上下文，长期记忆 | 不用重复解释，AI记得一切 |
| **智能归档** | 自动识别价值，静默保存 | 零负担积累知识库 |
| **知识图谱** | 语义关联，跨域连接 | 发现隐形联系，激发新洞察 |
| **多端统一** | 语音 + Terminal 共享记忆 | 不同场景，同一大脑 |
| **隐私可控** | 本地优先，可选云同步 | 数据完全拥有，合规无忧 |

### 2.2 可打断对话体验

**完整交互流程：**

```
┌─────────────────────────────────────────────────────────┐
│                   可打断对话流程                          │
└─────────────────────────────────────────────────────────┘

1. 用户说话
   ├─ 实时STT：边说边转文字
   ├─ 实时显示：用户看到转录内容
   └─ AI边听边理解

2. AI思考
   ├─ 停止录音
   ├─ 调用LLM生成回复
   └─ 流式返回文本

3. AI说话（TTS）
   ├─ 流式生成语音
   ├─ 实时播放
   └─ 同时监听：检测用户打断

4. 用户打断（随时）
   ├─ 检测到用户说话
   ├─ 立即停止TTS播放
   ├─ 保存当前上下文
   └─ 转到步骤1

5. 恢复对话
   ├─ AI理解打断意图
   ├─ 结合上下文继续
   └─ 无缝衔接
```

**关键技术点：**
- **双工音频：** 同时录音和播放
- **打断检测：** VAD（Voice Activity Detection）
- **上下文保持：** 打断后记住之前的内容
- **平滑切换：** 快速停止和开始，无杂音

### 2.3 用户旅程

```
┌─────────────────────────────────────────────────────────────────┐
│                         典型使用场景                             │
└─────────────────────────────────────────────────────────────────┘

早晨通勤（语音交互）
  │
  ├─ "Hey Memory，记录一下想法"
  │  └─ 快速捕捉灵感，自动归档
  │
  ├─ "继续我们昨天关于架构的讨论"
  │  └─ AI加载完整上下文，无缝继续
  │
  └─ "把今天的要点整理成笔记"
     └─ 自动生成结构化文档

工作中（Terminal）
  │
  ├─ 编程时直接调用记忆
  │  └─ "查看我之前关于认证系统的笔记"
  │
  ├─ 代码和知识库同步
  │  └─ AI理解项目上下文，智能补全
  │
  └─ commit时自动关联相关设计
     └─ 知识图谱自动更新

晚上回顾（语音）
  │
  ├─ "今天学了什么新东西？"
  │  └─ AI汇总今日知识积累
  │
  ├─ "搜索关于微服务的所有笔记"
  │  └─ 语义搜索，跨文件关联
  │
  └─ "生成本周知识汇总"
     └─ 自动整理，发送到邮箱
```

---

## 三、产品形态

### 3.1 交互形式

| 形式 | 使用场景 | 优先级 |
|------|----------|--------|
| **macOS App** | 桌面工作，语音交互 | P0 |
| **iOS/Android App** | 移动场景，随时对话 | P1 |
| **Terminal CLI** | 开发工作，快速操作 | P0 |
| **Web Dashboard** | 知识管理，可视化 | P2 |

### 3.2 部署模式

```
┌─────────────────────────────────────────────────────────────────┐
│                        部署选项                                 │
└─────────────────────────────────────────────────────────────────┘

本地模式（MVP）
  ├─ 完全离线运行
  ├─ 数据存在本地设备
  ├─ 无需联网（AI调用除外）
  └─ 适合：个人用户、隐私敏感

混合模式
  ├─ 本地存储 + 可选云同步
  ├─ 跨设备同步知识库
  ├─ 本地优先，云端备份
  └─ 适合：多设备用户、云备份需求

云端模式（未来）
  ├─ 托管服务
  ├─ 团队协作
  └─ 适合：企业用户、团队知识共享
```

---

## 四、差异化竞争

### 4.1 竞品对比

| 维度 | Voice Memory | Basic Memory | Mem0 | Siri/ChatGPT |
|------|--------------|--------------|------|--------------|
| **语音交互** | ✅ 核心功能 | ❌ 仅Terminal | ❌ 仅API | ⚠️ 有限 |
| **个人知识库** | ✅ RAG + 图谱 | ✅ Markdown图谱 | ✅ 向量记忆 | ❌ |
| **隐私控制** | ✅ 本地优先 | ✅ 本地优先 | ⚠️ 可配置 | ❌ 云端 |
| **持久记忆** | ✅ 跨会话 | ✅ 文件持久 | ✅ 数据库 | ❌ 单次 |
| **智能归档** | ✅ 自动触发 | ⚠️ 手动 | ⚠️ 需编程 | ❌ |
| **多端统一** | ✅ 语音+Terminal | ⚠️ MCP客户端 | ❌ | ⚠️ 不统一 |
| **开源** | ✅ 计划开源 | ✅ AGPL-3.0 | ✅ Apache-2.0 | ❌ |
| **自托管** | ✅ 支持 | ✅ 支持 | ✅ 支持 | ❌ |

### 4.2 核心差异

**1. 语音优先设计**
- 竞品：文字交互为主，语音是附加功能
- 我们：从唤醒词到完整对话，全程语音优先

**2. 静默智能归档**
- 竞品：需要手动保存或调用API
- 我们：AI自动识别有价值内容，静默归档

**3. 双向知识协作**
- 竞品：AI只能读取或只能写入
- 我们：人类和AI共同编辑、增强知识库

**4. 隐时随地**
- 竞品：需要特定设备或场景
- 我们：耳机一声唤，任何地方都能对话

---

## 五、商业模式

### 5.1 产品版本

| 版本 | 功能 | 价格 | 目标用户 |
|------|------|------|----------|
| **社区版（免费）** | 本地运行，基础功能，个人使用 | 免费 | 个人用户、开发者 |
| **Pro版（订阅）** | 云同步、高级功能、优先支持 | $9-15/月 | 专业用户、知识工作者 |
| **团队版（订阅）** | 多人协作、共享知识库、管理后台 | $29-49/用户/月 | 小团队、创业公司 |
| **企业版（定制）** | 私有部署、定制开发、企业支持 | 联系销售 | 大企业、政府机构 |

### 5.2 收入来源

```
订阅收入（主要）
  ├─ 个人Pro订阅
  ├─ 团队/企业订阅
  └─ 增值服务（高级AI模型、更多存储）

托管服务
  ├─ 云端托管版本
  └─ 企业私有部署服务

企业服务
  ├─ 定制开发
  ├─ 技术支持与培训
  └─ 知识图谱咨询
```

---

## 六、产品路线图

### Phase 1: 文本中介 MVP（4-7周）

**目标：** 验证核心交互，快速迭代

```
核心流程
  ✅ 语音唤醒（Porcupine）
  ✅ STT → 文本 → LLM → 文本 → TTS
  ✅ 基础对话（非打断）
  ✅ macOS App + Terminal CLI

记忆系统
  ✅ Markdown + SQLite 存储
  ✅ 简单上下文管理
  ✅ 基础归档

技术栈
  ├─ 唤醒词: Porcupine（免费版）
  ├─ STT: OpenAI Whisper API
  ├─ AI: Claude 3.5 Haiku
  ├─ TTS: OpenAI TTS
  └─ 存储: Markdown + SQLite

指标
  └─ 端到端延迟: 2-3秒
  └─ 准确度: STT >90%, 用户满意度 >70%
```

### Phase 2: 可打断交互（8-12周）

**目标：** 实现自然的可打断对话

```
核心增强
  ✅ 流式STT（实时转录）
  ✅ 流式TTS（边生成边播放）
  ✅ VAD打断检测
  ✅ 双工音频（同时录音播放）
  ✅ 上下文保持（打断后恢复）

智能特性
  ✅ 区分噪音和真实打断
  ✅ 打断意图理解
  ✅ 上下文无缝衔接
  ✅ 快速响应（<1秒）

技术栈升级
  ├─ STT: 添加本地Whisper（降低延迟）
  ├─ VAD: sherpa-onnx（本地检测）
  ├─ 音频: PyAudio双工模式
  └─ 优化: 流水线并行处理

指标
  └─ 打断响应: <1秒
  └─ 对话自然度: >80%
```

### Phase 3: 端到端语音（13-20周）

**目标：** 原生语音到语音，更低延迟

```
核心突破
  ✅ 端到端语音模型（GPT-4o Realtime风格）
  ✅ 音频直接输入输出
  ✅ <500ms超低延迟
  ✅ 更自然的语音交互

高级特性
  ✅ 语音情感理解
  ✅ 自然停顿和语调
  ✅ 多模态输入
  ✅ 实时语音翻译

技术栈
  ├─ 模型: OpenAI Realtime API / 自研
  ├─ 音频: WebRTC / 自有协议
  └─ 部署: 边缘计算优化

指标
  └─ 端到端延迟: <500ms
  └─ 用户满意度: >90%
```

### 跨设备支持（并行进行）

```
移动端（Phase 2+）
  ├─ iOS App（Swift + SwiftUI）
  ├─ Android App（Kotlin）
  └─ 跨设备知识同步

Web端（Phase 3）
  ├─ WebRTC语音支持
  ├─ 浏览器内VAD
  └─ PWA支持
```

---

## 七、成功指标

### 7.1 产品指标

| 指标 | 目标（6个月） | 测量方式 |
|------|---------------|----------|
| **日活用户（DAU）** | 1,000+ | 独立用户登录 |
| **用户留存** | 30% D7留存 | 用户行为分析 |
| **对话频次** | 5+ 对话/用户/天 | 对话记录统计 |
| **归档价值** | 80% 归档被查看 | 文档访问日志 |
| **响应速度** | <2s 端到端延迟 | 性能监控 |

### 7.2 业务指标

| 指标 | 目标（12个月） | 测量方式 |
|------|---------------|----------|
| **付费转化率** | 5-10% | 免费到付费转化 |
| **MRR** | $10,000+ | 订阅收入统计 |
| **NPS** | 50+ | 用户满意度调查 |
| **社区活跃度** | 500+ GitHub stars | 开源社区参与 |

---

## 八、风险与挑战

### 8.1 技术风险

| 风险 | 影响 | 缓解措施 |
|------|------|----------|
| **唤醒词误触发** | 用户体验差 | 多轮调优、敏感度配置 |
| **STT准确度不足** | 理解偏差 | 优选OpenAI Whisper、支持纠错 |
| **AI成本过高** | 商业不可持续 | 分层定价、本地模型降本 |
| **跨平台兼容** | 开发周期长 | 优先macOS、渐进扩展 |

### 8.2 市场风险

| 风险 | 影响 | 缓解措施 |
|------|------|----------|
| **大厂抄袭** | 竞争压力 | 开源社区、深耕细分 |
| **用户习惯难改** | 采用缓慢 | 渐进式引导、无痛迁移 |
| **隐私担忧** | 信任障碍 | 本地优先、代码开源 |

---

## 九、愿景

> **"成为每个人脑海的外接硬盘"**

短期（1年）：成为个人知识管理的首选语音工具
中期（3年）：构建开放的个人AI记忆生态
长期（5年）：重新定义人机交互，让AI真正成为"第二大脑"

---

**文档版本历史：**
- v0.1 (2025-12-29): 初始版本，概念定义
